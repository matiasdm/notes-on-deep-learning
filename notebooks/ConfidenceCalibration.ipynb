{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract:** The output of DNN tends to be extremelly overconfident about its predictions. In particular, when samples from a different (unknown) distribution are feeded into the network. As we will discuss in the first toy example, at least two problems manifest. First in the trasition between two classes, very flexible network tend to produce abrupt posterior predictions that do not match the actual posterior probabilities. Second, in regions where there is no data (i.e., far away from the support of the distributions of the target classes) the network tend to be extremelly overconfidente about predictions that actually noise. \n",
    "\n",
    "Refs:\n",
    "1. Thulasidasan, S., Bhattacharya, T., Bilmes, J., Chennupati, G., & Mohd-Yusof, J. (2019). Combating Label Noise in Deep Learning Using Abstention. \n",
    "2. Hendrycks, D., Mazeika, M., & Dietterich, T. (2018). Deep Anomaly Detection with Outlier Exposure.\n",
    "3. 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnn_tests",
   "language": "python",
   "name": "dnn_tests"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
